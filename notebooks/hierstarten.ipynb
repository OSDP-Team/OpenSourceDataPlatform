{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d262cec-1b63-48ab-904d-7a5be3e0ca5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.10\n",
      "openjdk 17.0.12 2024-07-16\n",
      "OpenJDK Runtime Environment (build 17.0.12+7-Ubuntu-1ubuntu224.04)\n",
      "OpenJDK 64-Bit Server VM (build 17.0.12+7-Ubuntu-1ubuntu224.04, mixed mode, sharing)\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.2\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 17.0.12\n",
      "Branch HEAD\n",
      "Compiled by user ubuntu on 2024-08-06T11:36:15Z\n",
      "Revision bb7846dd487f259994fdc69e18e03382e3f64f42\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "! python3 -V\n",
    "! java --version\n",
    "! pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d94633af-498f-41cb-aabd-1a071f0d5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get minio credentials\n",
    "with open(\"/minio-s3-credentials/accessKey\", \"r\") as f:\n",
    "    minio_user = f.read().strip()\n",
    "\n",
    "with open(\"/minio-s3-credentials/secretKey\", \"r\") as f:\n",
    "    minio_pwd = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533774d0-758c-4b48-b81c-13795ac80862",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbcd07-3fa8-4473-8bed-b59ea95ed033",
   "metadata": {},
   "source": [
    "## from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"local-with-s3a\")\n",
    "        .master(\"local[*]\")\n",
    "        # >>> fehlende Treiber+SDK holen\n",
    "        .config(\n",
    "            \"spark.jars.packages\",\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.688\"\n",
    "        )\n",
    "        # >>> MinIO-Parameter\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\",\n",
    "                \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\",  minio_user)\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\",  minio_pwd)\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")  # wenn MinIO nur HTTP spricht\n",
    "        .getOrCreate()\n",
    ")\n",
    "Spark\n",
    "Spark can be used in client mode (recommended for JupyterHub notebooks, as code is intended to be called in an interactive\n",
    "fashion), which is the default, or cluster mode. This notebook uses spark in client mode, meaning that the notebook itself\n",
    "acts as the driver. It is important that the versions of spark and python match across the driver (running in the juypyterhub image)\n",
    "and the executor(s) (running in a separate image, specified below with the `spark.kubernetes.container.image` setting).\n",
    "\n",
    "The jupyterhub image `quay.io/jupyter/pyspark-notebook:spark-3.5.2` uses a base ubuntu image (like the spark images).\n",
    "The versions of java match exactly. Python versions can differ at patch level, and the image used below `oci.stackable.tech/demos/spark:3.5.2-python311` is built from a `spark:3.5.2-scala2.12-java17-ubuntu` base image with python 3.11 (the same major/minor version as the notebook) installed.\n",
    "\n",
    "## S3\n",
    "As we will be reading data from an S3 bucket, we need to add the necessary `hadoop` and `aws` libraries in the same hadoop version as the\n",
    "notebook image (see `spark.jars.packages`), and define the endpoint settings (see `spark.hadoopo.fs.*`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f254645f-5b3e-4a75-bdc4-d25055df97d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=0), Row(id=1), Row(id=2)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"local-with-s3a\")\n",
    "        .master(\"local[*]\")\n",
    "        # Treiber/JARs\n",
    "        .config(\n",
    "            \"spark.jars.packages\",\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.688\"\n",
    "        )\n",
    "        # S3A / MinIO-Basics\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\",\n",
    "                \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\",  minio_user) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\",  minio_pwd) \\\n",
    "        # -------------------------------------------------\n",
    "        # >>> S3-tauglicher Committer (Fix f√ºr rename-Fehler)\n",
    "        # -------------------------------------------------\n",
    "        .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.committer.name\", \"directory\") \\\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.range(3).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278407f7-9671-431d-a877-763590d6d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   b|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"a\", 1), (\"b\", 2)], [\"col1\", \"col2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88050394-5f54-4730-82a8-c40159758d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: demo/gas-sensor/raw/20160930_203718.csv\n"
     ]
    }
   ],
   "source": [
    "# Manual S3 file check via pyarrow.fs\n",
    "import pyarrow.fs as fs\n",
    "\n",
    "s3 = fs.S3FileSystem(endpoint_override=\"http://minio:9000/\", access_key=minio_user, secret_key=minio_pwd, scheme=\"http\")\n",
    "files = s3.get_file_info(fs.FileSelector(\"demo/gas-sensor/raw/\", recursive=True))\n",
    "for f in files:\n",
    "    print(\"Found file:\", f.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461fb39c-828a-42e0-a2db-4faee5df678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------+--------+-------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|timesecs| coppm|humidity|temperature|flowrate|heatervoltage|     r1|     r2|     r3|     r4|     r5|     r6|     r7|     r8|     r9|    r10|    r11|    r12|    r13|    r14|\n",
      "+--------+------+--------+-----------+--------+-------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|  0.0000|0.0000| 49.7534|    23.7184|233.2737|       0.8993| 0.2231| 0.6365| 1.1493| 0.8483| 1.2534| 1.4449| 1.9906| 1.3303| 1.4480| 1.9148| 3.4651| 5.2144| 6.5806| 8.6385|\n",
      "|  0.3090|0.0000| 55.8400|    26.6200|241.6323|       0.2112| 2.1314| 5.3552| 9.7569| 6.3188| 9.4472|10.5769|13.6317|21.9829|16.1902|24.2780|31.1014|34.7193|31.7505|41.9167|\n",
      "|  0.6180|0.0000| 55.8400|    26.6200|241.3888|       0.2070|10.5318|22.5612|37.2635|17.7848|33.0704|36.3160|42.5746|49.7495|31.7533|57.7289|53.6275|56.9212|47.8255|62.9436|\n",
      "|  0.9260|0.0000| 55.8400|    26.6200|241.1461|       0.2042|29.5749|49.5111|65.6318|26.1447|58.3847|67.5130|68.0064|59.2824|36.7821|66.0832|66.8349|66.9695|50.3730|64.8363|\n",
      "|  1.2340|0.0000| 55.8400|    26.6200|240.9121|       0.2030|49.5111|67.0368|77.8317|27.9625|71.7732|79.9474|79.8631|62.5385|39.6271|68.1441|62.0947|49.4614|52.8453|66.8445|\n",
      "|  1.5440|0.0000| 55.8400|    26.6200|240.8361|       0.2020|60.1083|74.3444|81.5100|29.7970|72.9643|83.1477|80.5302|58.0412|39.2482|65.0981|67.8697|63.5316|50.3730|63.3641|\n",
      "|  1.8540|0.0000| 55.8400|    26.6200|240.7602|       0.2010|64.1020|74.3444|76.4748|28.1797|72.4181|78.4368|79.0768|59.7614|40.4067|64.1416|61.6173|60.5007|50.6424|66.2847|\n",
      "|  2.1630|0.0000| 55.8400|    26.6200|240.6845|       0.2009|62.6869|71.3877|73.8965|27.6523|64.4007|69.7912|72.5239|55.6363|39.6271|62.7987|65.8307|56.5195|50.3730|64.3090|\n",
      "|  2.4720|0.0000| 55.8400|    26.6200|240.6624|       0.2009|54.8336|67.0368|69.2228|27.4429|64.9211|63.9055|63.5872|54.0730|40.5881|64.1416|63.9090|53.0811|48.9146|63.3641|\n",
      "|  2.7810|0.0000| 55.8400|    26.6200|240.6519|       0.2003|51.2600|64.5303|61.4312|25.6881|54.7235|57.2118|59.6255|55.6363|40.9928|61.9071|60.3791|52.7888|48.3640|62.4461|\n",
      "|  3.0900|0.0000| 55.8400|    26.6200|240.6414|       0.2000|44.4420|54.4606|58.1498|25.4084|53.6815|54.5217|56.1218|60.6597|40.8079|58.9374|54.2965|51.5393|49.4775|64.8363|\n",
      "|  3.3970|0.0000| 55.8400|    26.6200|240.6238|       0.2000|41.4592|50.3972|52.5235|24.4668|51.7661|51.7707|51.7898|51.9752|40.5881|61.0401|62.4980|50.3466|47.2986|62.4461|\n",
      "|  3.7070|0.0000| 55.8400|    26.6200|240.6008|       0.2002|37.1649|48.7046|49.2128|21.9832|47.5412|47.8646|50.8483|54.0730|40.5881|58.1498|59.4820|50.3466|47.8255|63.8761|\n",
      "|  4.0150|0.0000| 55.8400|    26.6200|240.5780|       0.2000|33.3963|41.8611|48.6589|23.4223|47.2584|47.5609|47.2542|50.6634|40.4067|62.7987|61.6173|47.8274|46.0523|62.4461|\n",
      "|  4.3240|0.0000| 55.8400|    26.6200|240.5654|       0.2000|31.3207|37.6686|40.2600|23.2018|43.2625|43.9737|46.7415|59.7614|39.6271|56.9725|60.3791|47.3060|45.5628|61.5540|\n",
      "|  4.6350|0.0000| 55.8400|    26.6200|240.5726|       0.2000|28.0928|32.3820|43.0884|22.1194|43.9021|44.2341|45.2667|54.0730|40.0132|58.9374|55.3634|47.5890|46.0523|62.9436|\n",
      "|  4.9440|0.0000| 55.8400|    26.6200|240.5797|       0.2000|25.0092|26.7632|41.4265|22.6808|42.4121|43.9737|44.3331|54.8097|39.0446|53.8273|59.4820|48.1165|44.6138|60.6866|\n",
      "|  5.2540|0.0000| 55.8400|    26.6200|240.5751|       0.2000|22.0276|21.3459|36.9410|21.9218|41.1962|41.2963|43.6371|52.6564|40.1911|57.7289|54.6684|47.5890|45.7841|64.3090|\n",
      "|  5.5620|0.0000| 55.8400|    26.6200|240.4818|       0.2000|18.3814|16.2418|34.5448|22.0573|41.5938|41.2963|42.3453|51.0146|35.8154|57.7289|57.9038|45.8065|43.2606|61.5540|\n",
      "|  5.8710|0.0000| 55.8400|    26.6200|240.3881|       0.2000|15.1623|12.1369|29.0829|21.4542|40.0465|39.5062|43.0013|52.6564|39.6271|55.1962|54.2965|46.5671|44.1536|62.9436|\n",
      "+--------+------+--------+-----------+--------+-------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"s3a://demo/gas-sensor/raw/\", header = True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0e5d311-17b1-448b-b2db-832bedd6e26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295719"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.write.csv(\"s3a://demo/gas-sensor/rewritten/\", mode=\"overwrite\")\n",
    "df.write.parquet(\"s3a://demo/gas-sensor/parquet/\", mode=\"overwrite\")\n",
    "\n",
    "df2 = spark.read.parquet(\"s3a://demo/gas-sensor/parquet/\", header = True)\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02067b7-991a-421c-ac8f-096e3352ffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+--------+\n",
      "|hour|humidity|temperature|flowrate|\n",
      "+----+--------+-----------+--------+\n",
      "|   1|    55.4|      26.61|  240.05|\n",
      "|   2|   54.84|      26.61|  239.94|\n",
      "|   3|   54.38|      26.58|  239.98|\n",
      "|   4|   53.95|      26.58|  239.99|\n",
      "|   5|   53.82|      26.58|  240.02|\n",
      "|   6|   53.46|      26.58|  239.96|\n",
      "|   7|   53.25|      26.58|  239.98|\n",
      "|   8|   52.85|      26.58|  239.95|\n",
      "|   9|   52.81|      26.58|  240.03|\n",
      "|  10|   52.81|      26.57|  240.02|\n",
      "|  11|   52.81|      26.57|  239.98|\n",
      "|  12|   52.67|      26.58|  240.02|\n",
      "|  13|    52.3|      26.57|  240.03|\n",
      "|  14|    52.3|      26.57|  240.03|\n",
      "|  15|   52.29|      26.56|  239.97|\n",
      "|  16|   51.22|      26.55|  239.96|\n",
      "|  17|   49.02|      26.55|  239.98|\n",
      "|  18|   47.44|      26.55|  240.01|\n",
      "|  19|   46.51|      26.54|  239.97|\n",
      "|  20|   45.69|      26.54|  239.97|\n",
      "+----+--------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "\n",
    "df2 = df2.withColumn(\"hour\", (functions.floor(df2.timesecs / 60) + 1))\n",
    "\n",
    "dfs = df2.select(\n",
    "    df2.hour,\n",
    "    df2.humidity,\n",
    "    df2.temperature,\n",
    "    df2.flowrate\n",
    ").groupby(\"hour\").agg(\n",
    "    functions.round(functions.avg('humidity'), 2).alias('humidity'),\n",
    "    functions.round(functions.avg('temperature'), 2).alias('temperature'),\n",
    "    functions.round(functions.avg('flowrate'), 2).alias('flowrate')\n",
    ").orderBy(\"hour\")\n",
    "\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "564f6c3c-d64e-4263-93dc-30f9d0570db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.write.parquet(\"s3a://demo/gas-sensor/agg/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641b91a-f5ac-4960-b193-814f6b40e67e",
   "metadata": {},
   "source": [
    "### Convert between Spark and Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc645ce3-9317-47af-a88c-b176f6adc952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>humidity</th>\n",
       "      <th>temperature</th>\n",
       "      <th>flowrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55.40</td>\n",
       "      <td>26.61</td>\n",
       "      <td>240.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>54.84</td>\n",
       "      <td>26.61</td>\n",
       "      <td>239.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54.38</td>\n",
       "      <td>26.58</td>\n",
       "      <td>239.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>53.95</td>\n",
       "      <td>26.58</td>\n",
       "      <td>239.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>53.82</td>\n",
       "      <td>26.58</td>\n",
       "      <td>240.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>53.46</td>\n",
       "      <td>26.58</td>\n",
       "      <td>239.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>53.25</td>\n",
       "      <td>26.58</td>\n",
       "      <td>239.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>52.85</td>\n",
       "      <td>26.58</td>\n",
       "      <td>239.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>52.81</td>\n",
       "      <td>26.58</td>\n",
       "      <td>240.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>52.81</td>\n",
       "      <td>26.57</td>\n",
       "      <td>240.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour  humidity  temperature  flowrate\n",
       "0     1     55.40        26.61    240.05\n",
       "1     2     54.84        26.61    239.94\n",
       "2     3     54.38        26.58    239.98\n",
       "3     4     53.95        26.58    239.99\n",
       "4     5     53.82        26.58    240.02\n",
       "5     6     53.46        26.58    239.96\n",
       "6     7     53.25        26.58    239.98\n",
       "7     8     52.85        26.58    239.95\n",
       "8     9     52.81        26.58    240.03\n",
       "9    10     52.81        26.57    240.02"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = dfs.toPandas()\n",
    "df_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d57660-7f95-4d37-9768-c2c58202b68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+--------+\n",
      "|hour|humidity|temperature|flowrate|\n",
      "+----+--------+-----------+--------+\n",
      "|   1|    55.4|      26.61|  240.05|\n",
      "|   2|   54.84|      26.61|  239.94|\n",
      "|   3|   54.38|      26.58|  239.98|\n",
      "|   4|   53.95|      26.58|  239.99|\n",
      "|   5|   53.82|      26.58|  240.02|\n",
      "|   6|   53.46|      26.58|  239.96|\n",
      "|   7|   53.25|      26.58|  239.98|\n",
      "|   8|   52.85|      26.58|  239.95|\n",
      "|   9|   52.81|      26.58|  240.03|\n",
      "|  10|   52.81|      26.57|  240.02|\n",
      "|  11|   52.81|      26.57|  239.98|\n",
      "|  12|   52.67|      26.58|  240.02|\n",
      "|  13|    52.3|      26.57|  240.03|\n",
      "|  14|    52.3|      26.57|  240.03|\n",
      "|  15|   52.29|      26.56|  239.97|\n",
      "|  16|   51.22|      26.55|  239.96|\n",
      "|  17|   49.02|      26.55|  239.98|\n",
      "|  18|   47.44|      26.55|  240.01|\n",
      "|  19|   46.51|      26.54|  239.97|\n",
      "|  20|   45.69|      26.54|  239.97|\n",
      "+----+--------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.createDataFrame(df_pandas)\n",
    "spark_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
